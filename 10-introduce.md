****
# 1. Графы знаний <a name="1"></a>


## 1.1. Актуальность создания эффективных программных и аппаратных средств обработки графов <a name="1_1"></a>

> Граф – множество вершин *X*, на элементах которого определены двуместные **отношения смежности** (ребра) – *(x<sub>j</sub>, x<sub>i</sub>) <em>&isin;</em>F* , где *x<sub>j</sub>*,*x<sub>i</sub> <em>&isin;</em> X*. Тогда пара вершин, находящихся в отношении смежности, рассматривается как ребро *u<sub>k</sub> = (x<sub>j</sub>, x<sub>i</sub>), u<sub>k</sub> <em>&isin;</em> U*.

На элементах множеств X и U определены также **отношения смежности** *F<sub>1</sub>(X,X)* и *F<sub>2</sub>(U,U)*. Например, вершине *x<sub>i</sub>* смежна вершина *x<sub>k</sub>*, если существует ребро *u<sub>j</sub>*, **инцидентное** *x<sub>i</sub>*, такое, что *x<sub>k</sub>* **инцидентно** ему. Аналогично ребру *u<sub>j</sub>* смежно ребро *u<sub>l</sub>*,если существует вершина *x<sub>i</sub>*, **инцидентная ребру** *u<sub>j</sub>*, такая, что *u<sub>l</sub>* **инцидентна** этой вершине.

Существует несколько видов графов, отличающихся свойствами предикатов инцидентности – *неориентированные, ориентированные, гипер- и ультраграфы, метаграфы*. 


![Рисунок 1 — Виды графов](assets/graphs.jpg)
**Рисунок 1 — Виды графов**

Графы знаний является способом представления модели знаний в виде графовой структуры. Технологии представления и обработки знаний в виде графов приобрели большое значение во многих областях, в которых другие методы показали низкую эффективность. Благодаря способности сохранять информацию о различных объектах и явлениях и учитывать связи между ними, графы знаний могут использоваться при анализе больших данных в биоинформатике \[[1](https://www.researchgate.net/publication/51083566_Using_graph_theory_to_analyze_biological_networks)\], в персонифицированной медицине, системах безопасности городов \[[2](https://www.researchgate.net/publication/349236112_A_Metamodel_and_Framework_for_Artificial_General_Intelligence_From_Theory_to_Practice)\]\[[3](https://www.researchgate.net/publication/335444390_A_reasoning_based_model_for_anomaly_detection_in_the_Smart_City_domain)\]\[[4](https://www.researchgate.net/publication/321306827_A_Survey_on_Network_Embedding)\]\[[5](https://cs.stanford.edu/people/jure/pubs/graphrepresentation-ieee17.pdf)\]\[[6](https://www.researchgate.net/publication/332881469_ConceptNet_55_An_Open_Multilingual_Graph_of_General_Knowledge)\], в компьютерных сетях, финансовом секторе, при контроле сложного промышленного производства, для анализа информации социальных сетей и во многих других областях.  

Существенное влияние на эффективность применения аппаратных средств в задачах обработки графов оказывает адекватность их применения в рамках парадигмы рассматриваемой вычислений. Так, ряд задач обработки графов основан на статических графах, изменение которых либо не предусматривается вообще, либо происходит за пределами графового вычислителя. Для такого класса задач обработки графов характерным является этап передачи графа из исходного места хранения в оперативное хранилище графового вычислителя или же потоковая обработка. Подобная обработка  позволяет применять классические варианты построения вычислительных систем, в которых передача данных происходит большими или непрерывными пакетами, а останов ритмичной обработки не предусматривается спецификой алгоритмов. Для решения данного класса задач хорошо зарекомендовали себя графические ускорители GPU \[[7](http://infolab.dgist.ac.kr/~mskim/papers/SIGMOD16.pdf)\] и матрично-конвейерные структуры на ПЛИС \[[8](https://readingxtra.github.io/docs/graph-fpga/07544758.pdf)\]. 
Второй вариант постановки задач обработки графов отличается тем, что информация графа должна меняться как под воздействием внутренней обработки (например, результата поиска кратчайшего пути или центральных вершин), так и под воздействием внешних факторов (запросов на изменение информации графов). В этом случае граф должен находится непосредственно в оперативной памяти (памяти процессора общего назначения или специального устройства обработки графов). Такой вариант предполагает непрерывность процессов обработки и изменения, что приводит к необходимости применения иных архитектурных принципов.  Вычислительные средства, эффективно воплощающие подобную функциональность, опираются на оптимизацию алгоритмов доступа к структурам данных и графам в памяти, на повышение эффективности подсистемы памяти, на увеличение степени параллельность при обработке каждой нити вычислений. 

Приведенные выше различия статических и динамических задач обработки графов приводят к тому, что несмотря на большое количество и разнообразие средств вычислительной техники, потребность в высокопроизводительных ЭВМ для решения задач обработки графов знаний, чрезвычайно высока. При решении подобных задач дальнейшее увеличение скорости обработки на основе универсальных микропроцессорных систем трудно достижимо.  Даже благодаря высокому уровню параллелизма, глубокой конвейеризации и большим тактовым частотам современные микропроцессоры и графические ускорители не способны эффективно решать проблемы обработки больших графов. Сказываются такие фундаментальные проблемы, как: зависимости по данным \[[9](https://www.proquest.com/openview/de3e03b91bb1bf566067b332f3012c96/1?pq-origsite=gscholar&cbl=2029993)\]\[[10](https://www.semanticscholar.org/paper/Visualizing-effect-of-dependency-in-superscalar-Patel-Kumar/468bc56b8ee79301b4930451c28bfdb8c579d899)\]; необходимость распределения вычислительной нагрузки при обработке нерегулярных графов; наличия конфликтов при доступе к памяти большого количества обрабатывающих ядер \[[11](http://cdn.iiit.ac.in/cdn/cvit.iiit.ac.in/papers/Pawan07accelerating.pdf)\]. 

В МГТУ им. Н.Э.Баумана в настоящее время создается вычислительный комплекс, предназначенного для обработки графов и обладающего передовыми техническими характеристиками: аппаратная реализация набора команд дискретной математики, гетерогенная архитектура, хранение и обработка до 1 триллиона вершин графа. В ходе практикума все участникам будет предоставлен доступ к одной карте комплекса Тераграф.


## 1.2. Пименения графов в задачах аналитики данных и искусственном интеллекте <a name="1_2"></a>

Безусловным достижением последнего десятилетия является внедрение систем анализа данных на основе алгоритмов и методов машинного обучения. Эти технологии позволяют решить одну из важнейших задач: выявление фактов из огромного потока данных. Следующим звеном в цепи интеллектуального анализа данных должна будет система, способная хранить и обрабатывать найденные  факты и связи между ними в виде графов знаний (рисунок 2). 


![Рисунок 2 — Аналитическая система на основе графов знаний](assets/ai_analytics.jpg)
**Рисунок 2 — Аналитическая система на основе графов знаний**

Уже сейчас оказывается недостаточным просто хранить огромные массивы фактов и извлекать их по запросу. Необходимо иметь систему, способную анализировать причинно-следственные связи между событиями, оценивать достоверность и полноту сведений, выявлять и хранить контекстную информацию.  Именно графы позволяют получать ответы на те вопросы, которые интересуют пользователя такой системы. Например, насколько вероятно развитие дорожной обстановки по неблагоприятному сценарию и как его избежать, имеют ли место незаконные финансовые операции, кто в них задействован и какие схожие сценарии возможны? При этом подход к формированию ответов на такие вопросы должен принципиально отличаться от простого поиска ситуаций похожих на те, что система видела раньше (как это делается сейчас в нейронных сетях).  Аналитическая система будущего должна не просто искать сходства и различия, но уметь логически рассуждать на основе графов знаний. В этом смысле, все известные системе факты и правила будут использоваться для принятия решения. Использование логического вывода на основе графов способно избавить аналитическую систему от ошибок, связанных с игнорированием контекста и здравого смысла, и, главное, делает результат объяснимым. 


![Рисунок 3 — Примеры графов знаний](assets/knowledge_graphs.jpg)
**Рисунок 3 — Примеры графов знаний**

Важность повышения эффективности алгоритмов на графах и совершенствования вычислительных средств для их реализации привели к появлению такого направления как **Graph Data Science**. Это подразумевает выделение в отельную научную область инересов всего, что связано с аналитикой данных с использованием графов. В набор средств анализа входят такие алгоритмы, как: обнаружение сообществ в графах, центральность, поиск подобия и изоморфизм, поиска кратчайших путей и максимлаьных потоков, и ряд других. Приведем некоторые примеры применения графовых алгоритмов для решения важных практических задач.


**Обнаружение незаконных финансовых операций** 

Мошенники делят грязные деньги на множество малых частей и смешивают их с законными средствами, и затем превращают их в легальные активы. Для этого используется круговое движение денежных средств, которое скрывает первоначальный источник за длинной цепочкой транзакций. Графы позволяют построить модель движения денег для таких мошеннических схем и своевременно препятствовать им.

**Обнаружение финансового мошенничества в реальном времени**

Все банки стремятся ускорить доступ клиентов к услугам и денежным переводам, что представляет собой потенциальную опасность. Необходимо анализировать множество факторов, сопровождающих транзакцию: местоположение клиента и ip адрес устройства; расстояние до предыдущего места нахождения клиента и время, прошедшее с этого момента; номер карты и счета клиента; история расходов клиента по карте и многое другое. Эти данные формируют графовую структуру, позволяя банку оценить отношения событий и имеющихся в его распоряжении данных.

**Промышленное производство и контроль жизненного цикла оборудования**

Современное промышленное производство основано на длинных цепочках поставок. При этом сроки поставок и жизненный срок изделий различных поставщиков может отличаться. Если представить масштабы работы таких технически сложных объектов, как энергосеть региона или даже страны, становится понятным сложность планирования и модернизации их работы. Необходимо учесть взаимное влияние возможных отказов оборудования, сложность и стоимость их замены, гарантийный срок службы и т.д. Графы позволяют создать модель таких сложных систем и осуществлять управление ими.

**Персонифицированная медицина**

Графы хорошо подходят для хранения и визуализации медицинской информации. Данные о состоянии организма пациента являются взаимосвязанными, и могут быть соотнесены с аналогичными данными других пациентов. Компания AstraZeneca провела успешные исследования, в которых граф знаний об организме одного члена “сообщества” использовался при выборе терапии для больного по аналогии с другими подобными случаями.

**Биомедицинские исследования**

Цепочки химических реакций также представимы в виде графов, в связи с чем в биологии и биомедицине стоит проблема моделирования подобных структур. Обмен веществ в организме человека - это также сеть химических реакций, катализируемых ферментами. В настоящее время изучены более 10 тысяч различных химических реакций, которые происходят в организме для построения клеточных структур. С помощью графов можно описать метаболизм как круговорот атомов, представив в них все реакции с химической структурой небольших соединений (метаболитов). Это, в свою очередь, открывает перед исследователями возможности синтеза новых лекарственных препаратов. 

**Контроль мошеннической деятельности в налоговой сфере**

Системы налогообложения и выявление неуплаты налогов должны постоянно совершенствоваться, чтобы учитывать новые способы ухода от уплаты налогов и мошеннические схемы. С повышение доступности предпринимательства и автоматизацией бизнес-деятельности у преступников появились дополнительные возможности создания подставных юридических лиц, через которых передаются незаконно полученные денежные средства. Графы позволяют анализировать сложные схемы использования подставных юридических лиц и обнаружить мошенническую схему по структуре и взаимосвязи субъектов, участвующих в бизнес-деятельности. Подозрительные паттерны быстро обнаруживаются и выявляются структурные закономерности, которые позволяют установить единый центр мошеннической деятельности.


## 1.3. Переход от реляционной к графовой форме представления информации <a name="1_3"></a>


# 2. Структура микропроцессора Леонард Эйлер и вычислительного комплекса Тераграф <a name="2"></a>

Анализ графов существенно отличается от привычной арифметико-логической обработки. Самыми существенными особенностями алгоритмов обработки графов являются: 

*  зависимости по данным между последовательными итерациями поиска и анализа информации

*  большее количество операций доступа к памяти по сравнению с количеством арифметико-логических операций. 

Поэтому в МГТУ им. Н.Э.Баумана была разработана специальная гетерогенная архитектура вычислительный комплекс `Тераграф`, учитывающий особенности обработки графов. Отличительными чертами комплекса являются:

1.  Доступ к графам и их обработку осуществляет специализированный микропроцессор `lnh64` с набором команд дискретной математики (`Discrete mathematics instruction set computer`, `DISC`).

2.  Оперативное хранилище графов (так называемая `Локальная память структур`, `Local Structure Memory`, `LSM`) имеет большой размер (2.5 ГБ на один микропроцессор `lnh64`) и организована как ассоциативная память.

3.  `DISC` микропроцессор `lnh64` подключен непосредственно к шине памяти малого арифметического процессора `riscv32im`. Пара процессоров `lnh64` и `riscv32im` и составляет гетерогенное ядро обработки графов (`Graph Processing Core`, `GPC`).

4.  Множество гетерогенных ядер обработки графов `GPC` составляют многоядерный микропроцессор `Леонард Эйлер`.


Рассмотрим структуру комплекса Тераграф более подробно.


## 2.1. Набор команд дискретной математики <a name="2_1"></a>



Ключевым вопросом при проектировании любого программно-управляемого устройства является выбор набора команд. Так как целями создания микропроцессорного ядра lnh64 являются аппаратная поддержка дискретной математики, набор инструкций составлен на основе таких понятий, как кванторы, отношения и операции над множествами.

**Таблица 1 – Соответствие инструкций DISC функциям, кванторам и операциям дискретной математики**

| Функции, кванторы и операции дискретной математики | Инструкции набора команд DISC |
|----------------------------------------------------|-------------------------------|
| Функция хранения кортежа                           | INS                           |
| Функция отношения элементов множества              | NEXT,PREV,NSM,NGR,MIN,MAX     |
| Мощность множества                                 | CNT                           |
| Функция принадлежности элемента множеству          | SRCH                          |
| Добавление элемента в множество                    | INS                           |
| Исключение элемента из множества                   | DEL,DELS                      |
| Исключение подмножества из кортежа                 | DELS                          |
| Включение подмножества в кортеж                    | INS,LS,GR,LSEQ,GREQ,GRLS      |
| Отношение эквивалентности множеств                 | INS,LS,GR,LSEQ,GREQ,GRLS      |
| Объединение множеств                               | OR                            |
| Пересечение множеств                               | AND                           |
| Разность множеств                                  | NOT                           |


Последняя версия набора команд DISC состоит из 21 высокоуровневой инструкции, перечисленных ниже:

*  Search (*SRCH*) выполняет поиск значения, связанного с ключом.

*  Insert (*INS*) вставляет пару ключ-значение в структуру. SPU обновляет значение, если указанный ключ уже находится в структуре.

*  Операция Delete (*DEL*) выполняет поиск указанного ключа и удаляет его из структуры данных.

*  Последняя версия набора команд была расширена двумя новыми инструкциями (*NSM* и *NGR*) для обеспечения требований некоторых алгоритмов. Каждая инструкция набора включает до трех операндов: ключа, значения и номера структуры данных. Команды *NSM/NGR* выполняют поиск соседнего ключа, который меньше (или больше) заданного и возвращает его значение. Операции могут быть использованы для эвристических вычислений, где интерполяция данных используется вместо точных вычислений (например, кластеризация или агрегация).

*  Maximum /minimum  (*MAX, MIN*) ищут первый или последний ключи в структуре данных.

*  Операция Cardinality (*CNT*) определяет количество ключей, хранящихся в структуре.

*  Команды *AND, OR, NOT* выполняют объединения, пересечения и дополнения в двух структурах данных.

*  Срезы (*LS, GR, LSEQ, GREQ, GRLS*) извлекают подмножество одной структуры данных в другую.

*  Переход к следующему или предыдущему (*NEXT, PREV*) находят  соседний (следующий или предыдущий) ключ в структуре данных относительно переданного ключа. В связи с тем, что исходный 
ключ должен обязательно присутствовать в структуре данных, операции *NEXT/PREV* отличаются от *NSM/NGR*.

*  Удаление структуры (*DELS*) очищает все ресурсы, используемые заданной структурой.

*  Команда Squeeze (*SQ*) дефрагментирует блоки локальной памяти, используемые структурой. 

*  Команда Jump (*JT*) указывает код ветвления, который должен быть синхронизирован с хост CPU (команда доступна только в режиме МКОД).

Вызов команд lnh64 осуществляется передачей из микропроцессора riscv32im операндов и кода операции. Результаты выполнения команд сохраняются в регистрах резельтата (ключ и значение) и регистре статуса. Дополнительно предусмотрена очередь результатов, содержащая аналогичные данные, расположенные последовательно в порядке завершения инструкций. Другим способом передачи результата являются так называемые регистры mailbox.

>  Механизм ожидания результатов mailbox предполагает наличие регистров, чтение данных из которых возможно только при поступлении в них действительных значений результатов. В случае, если регистр не содержит результатов, чтение из него вызывает ошибку доступа, или же приостанавливает транзакцию на шине. После прочтения, результат регистра mailbox аннулируется (сбрасывается флаг достоврености).

Примеры вызова команд и ожидания результатов будут рассмотрены в практической части работы.


## 2.2. Структура вычислительного комплекса Тераграф <a name="2_2"></a>


Комплекс «Тераграф» предназначен для хранения и обработки графов сверхбольшой размерности и будет применяться для моделирования биологических систем, анализа финансовых потоков в режиме реального времени, для хранения знаний в системах искусственного интеллекта, создания интеллектуальных автопилотов с вункциями анализа дорожной обстановки, и в других прикладных задачах. Он способен обрабатывать графы сверхбольшой размерности до 10<sup>12</sup> (одного триллиона) вершин и 2·10<sup>12</sup> ребер. Комплекс состоит из 3-х однотипных гетерогенных узлов, которые взаимодействуют между собой через высокоскоростные сетевые подключения 100Gb Ethernet. Каждый узел состоит из хост-подсистемы, подсистемы хранения графов, подсистемы коммутации узлов, а также подсистемы обработки графов. Структурная схема одного узла представлена на рисунке 4


![Рисунок 4 — Структура гетерогенного узла](assets/Teragraph.jpg)
**Рисунок 4 — Структура гетерогенного узла**

Были выбраны следующие принципы организации комплекса:

*   для ускорения обмена данными внутри комплекса устройства обработки множеств и структур данных размещаются на одном кристалле с универсальными процессорным устройством;

*   совместно на одном кристалле размещается несколько DISC устройств, которые имеют независимые каналы памяти;

*   несколько гетерогенных вычислительных узлов объединены в единый комплекс высокоскоростными сетевыми интерфейсами, обеспечивающими взаимодействие DISC диск устройств и подсистемы памяти.

### 2.2.1. Хост-подсистема <a name="2_2_1"></a>

Основная вычислительная системы (так называемая хост-подсистема) берет на себя управления запуском вычислительных задач, поддержкой сетевых подключений, обработкой и балансировкой нагрузки. В хост-подсистему входят два многоядерных ЦПУ по 26 ядер каждый, оперативная память на 1 Тбайт и дополнительная энергонезависимая память на 8 Тбайт, где хранятся атрибуты вершин и ребер графа, буферизируются поступающие запросы на обработку и визуализацию графов, хранятся временные данные об изменениях в графах. В хост-подсистеме используется процессор с архитектурой x86 для обеспечения сетевого взаимодействия и связи системы с внешним миром. В функции хост-подсистемы входят:

*   на стадии инициализации комплекса: настройка сетевой подсистемы, подсистем хранения и обработки графов;

*   на стадии создания/изменения графов в локальной памяти подсистемы обработки графов: реализация очередей запросов на вставку/изменения, балансировка запросов к DISC системам, выделение и освобождение структур, контроль выполнения операций изменения;

*   на стадии запуска алгоритмов оптимизации: буферизация запросов оптимизации, инициализация процедур обработки, их запуск и контроль исполнения;

*   на стадии визуализации графов: буферизация запросов на визуализацию, настройка процедур формирования представлений графов для пользовательских процессов, запуск формирования 
представлений и контроль результатов, буферизация и передача представлений или изменений в представлениях пользовательским процессам.

Указанные функции реализованы в Программном ядре хост-подсистемы (host software kernel) – программном обеспечении, взаимодействующим с подсистемой обработки графов через шину PCIe. 

### 2.2.2. Подсистема хранения графов <a name="2_2_2"></a>

В подсистему хранения графов входят основная память 30Тбайт, состоящая из четырех NVMe SSD дисков по 7,7 Тбайт каждый. Технология NVMe (Non-Volatile Memory Express) обеспечивает интерфейс связи с увеличенной полосой пропускания, что повышает производительность и эффективность обработки графов.

### 2.2.3. Подсистема коммутации узлов <a name="2_2_3"></a>
 
Подсистема коммутации узлов представляет собой два сетевых модуля связи по протоколу 100Gb Ethernet, позволяющих организовать соединение каждого гетерогенного узла с каждым другим узлом комплекса. Шина PCIe обеспечивает высокопроизводительное взаимодействие хост-подсистемы с процессорами Леонард Эйлер, а также последних с подсистемой хранения графов.

### 2.2.4. Подсистема обработки графов <a name="2_2_4"></a>
 
Подсистема обработки графов каждого узла комплекса состоит из 3-х или 4-х карт (в зависимости от версии комплекса) многоядерных микропроцессоров Леонард Эйлер, каждый из которых в свою очередь включает 3 или 4 группы гетерогенных ядер (так называемых Core Groups, CG). В каждую такую группу входят от 2-х до 6-ти ядер DISC GPC, обладающих следующими характеристиками: объем доступной локальной памяти для хранения графов - до 2.5 Гбайт; разрядность ключей и значений - 64 бита; количество хранимых ключей и значений - до 117 миллионов; количество одновременно хранимых структур в локальной памяти структур - до 7;  объем ОЗУ CPE - 64 КБайт. Взаимодействие гетерогенных DISC ядер и хост-подсистемы осуществляется как через FIFO буферы, так и через адресуемую Глобальную память (Global memory, GM) размером 128 Кбайт, что позволяет выбирать наиболее эффективный механизм взаимодействия. 

Таким образом, комплекс «Тераграф» может содержать до 288 гетерогенных ядра DISC GPC, и хранить в оперативном доступе (в локальной памяти подсистемы обработки графов) до 11 миллиардов вершин. Группа ядер Core Group содержит разделяемый между гетерогенными ядрами контроллеры памяти, которые обеспечивает взаимодействие между GPC и Локальной памятью структур типа DDR4, а также с Глобальной памятью. Один и тот же блок Глобальной памяти используется всеми гетерогенными ядрами группы для передачи данных внутри группы и обмена данными с хост-подсистемой. 

Структурная схема микропроцессора Леонард Эйлер версии 4 представлена на рисунке 5.
В качестве единицы передаваемых данных принят блок размером 4КБ, который передается между хост-подсистемой и группой ядер CG с помощью механизмов прямого доступа к памяти.


![Рисунок 5 — Структура микропроцессора Леонард Эйлер ](assets/Leonhard_v4.jpg)
**Рисунок 5 — Структура микропроцессора Леонард Эйлер**


## 2.3. Микроархитектура гетерогенного ядра обработки графов <a name="2_3"></a>

Как было отмечено ранее, обработка графов в системе Тераграф выполняется на многочисленных гетерогенных ядрах, состоящих из двух микропроцессоров: CPE и SPE (см. рисунок 6). При этом CPE является универсальным RISC  ядром с арифметическим набором команд, в то время как SPE реализует набором команд дискретной математики. 
Каждый вычислительный элемент CPE состоит из очереди команд, блока выборки, блока декодирования команд, модуля предсказания переходов, арифметико-логического устройства, устройства доступа в память, интерфейса AXI4MM, блока ветвлений и интерфейса шины ускорителя AXL. Также вычислительный элемент связан шиной памяти с ПЗУ, в которой записан загрузчик, обеспечивающий передачу программных ядер. Для размещения программ и данных, каждый CPE имеет оперативную память размером 64КБ.


![Рисунок 6 — Структура ядра обработки графов](assets/Graph_Processing_Core.jpg)
**Рисунок 6 — Структура ядра обработки графов**

Под управлением поступающих из хост-подсистемы команд SPE выполняет хранение ключей и значений в многоуровневой подсистеме памяти, выполняет поиск, изменение и выдачу информации другим устройствам комплекса. Для ускорения поиска и обработки всего набора команд микропроцессор использует внутреннее представление множеств в виде B+деревьев, для которых возможна параллельная обработка нескольких вершин дерева как на промежуточных уровнях, используемых для поиска,так и на нижнем уровне, хранящем непосредственно ключи и значения.



## 2.4. Принципы взаимодействия микропроцессора Леонард Эйлер и Хост подсистемы<a name="2_4"></a>

Основу взаимодействия подсистем при обработке графов составляет передача блоков данных и коротких сообщений между GPC и хост-подсистемой. Для передачи сообщений для каждого GPC реализованы два аппаратных FIFO буфера на 512 записей: Host2GPC для передачи от хост-подсистемы к ядру, и GPC2Host для передачи в обратную сторону.

Обработка начинается с того, что собранное программное ядро (software kernel) загружается в локальное ОЗУ одного или нескольких CPE. Для этого используется механизм прямого доступа к памяти со стороны хост-подсистемы. В свою очередь, GPC (один или несколько) получают сигнал о готовности образа software kernel в Глобальной памяти, после чего вызывается загрузчик, хранимый в ПЗУ CPE. Загрузчик выполняет копирование программного ядра из Глобальной памяти в ОЗУ CPE и передает управление на начальный адрес программы обработки. Предусмотрен режим работы GPC, при котором во время обработки происходит обмен данными и сообщениями. Эти два варианта работы реализуется через буферы и очереди соответственно. На рисунке 7 представлена диаграмма последовательностей первого сценария работы – вызов обработчика с передачей параметров и возвратом значения через очередь сообщений.


![Рисунок 7 — Диаграмма последовательностей вызова обработчика с передачей параметров и возвратом значения через очередь сообщений](assets/io1.jpg)
**Рисунок 7 — Диаграмма последовательностей вызова обработчика с передачей параметров и возвратом значения через очередь сообщений**


Если код программного ядра уже загружен в ОЗУ CPE, хост-подсистема может вызвать любой из содержащихся в нем обработчиков. Для этого в GPC передает оговоренный UID обработчика (handler), после чего передается сигнал запуска (сигнал START). В ответ CPE устанавливает состояние BUSY и начинает саму обработку. В ходе обработки ядро может обмениваться сообщениями с хост-подсистемой через очереди (команды mq_send и mq_receive). По завершении обработки устанавливается состояние DONE и вырабатывается прерывание, которое перехватывается хост-подсистемой. Далее, пользовательское приложение хост-подсистемы уведомляется о завершении обработки и готовности результатов.

Если во время работы программному ядру software kernel требуется осуществить передачу больших блоков данным между CPE и хост-подсистемой, то может быть задействована Глобальная память и внешняя память большого размера (External Memory, до 16ГБ). Указанные варианты взаимодействия предполагает выделение и освобождение буферов и передачу указателей на них от хост-подсистемы к CPE. Соответствующая диаграмма последовательностей представлена на рисунке 8. 

![Рисунок 8 — Диаграмма последовательностей вызова обработчика с передачей параметров и возвратом значения через буфер в памяти сообщений](assets/io2.jpg)
**Рисунок 8 — Диаграмма последовательностей вызова обработчика с передачей параметров и возвратом значения через буфер в памяти сообщений**


На приведенном примере хост-подсистема передает информацию (ключ поиска) через буфер в глобальную память. Далее, происходит процесс инициализации асинхронной работы путем отправки UID обработчика в GPC (handler UID). Далее хост-подсистема может продолжить передачу данных, получая подтверждения от процесса обработчика GPC через очередь сообщений. Как правило, обработчик, запущенный в CPE, активно вызывает DISC команды SPE, передает данные и получает результаты от SPE в хост-подсистему. В конце работы, обработчик передает результаты обратно в хост-подсистему с использованием очереди сообщений или Глобальной памяти.


## 2.5. Библиотека [leonhard x64 xrt](https://gitlab.com/leonhard-x64-xrt-v2) <a name="2_5"></a>


Библиотека [leonhard x64 xrt](https://gitlab.com/leonhard-x64-xrt-v2) представляет собой API системного уровня, реализующего функциональные возможности по инициализации подсистемы обработки графов и взаимодействию с микропроцессором lnh64 DISC. Библиотека разделена на две части, представленные в таблице 2.

**Таблица**

| Раздел библиотеки                                                     | Описание                                                    | Язык программирования | Архитектура, Компилятор |
|-----------------------------------------------------------------------|-------------------------------------------------------------|-------------------------------------------------|
| [Host Lib](https://gitlab.com/leonhard-x64-xrt-v2/libraries/host-lib) |   Управления Хост подсистемой и взаимодействие с ядрами GPC | C++, объектная модель | x86, g++                |
| [SW Kernel Lib](https://gitlab.com/leonhard-x64-xrt-v2/libraries/sw-kernel-lib) | Взаимодействие с микропроцессором lnh64           | C, процедурная модель | riscv32, g++            |

Функциональные возможности Host Lib:

*   Настройка параметов в зависимости от версии микропроцесора Леонард Эйлер. 

*   Начальная инициализация аппаратной части, конфигурирование ПЛИС микропроцессором Леонард Эйлер. 

*   Инициализация буферов и очередей для взаимодействия Хост подсистемы и подсистемы обработки графов (ядер обработки графов GPC).

*   Передача и прием данных и сообщений к/от GPC через общую память и аппаратные очереди.

Функциональные возможности SW Kernel Lib:

*   Обмен данными и сообщениями с Хост-подсистемой.

*   Установка состояние sw_kernel (IDLE,BUSY).

*   Запуск обработчиков на ядре riscv32im.

*   Передача опрандов и кодов операций в микропроцессор lnh64.

*   Контроль результатов исполнения DISC команд.


Ниже приведен пример кода программы Хост подсистемы, выполняющей инициализацию и измерение тактовой частоты GPC.

```
	//Общесистемные библиотеки
	#include <iostream>
	#include <stdio.h>
	#include <stdexcept>
	#include <iomanip>
	#include <unistd.h>
	#include <sys/time.h>
	//Библиотеки Xilinx Runtime Library
	#include "experimental/xrt_device.h"
	#include "experimental/xrt_kernel.h"
	#include "experimental/xrt_bo.h"
	#include "experimental/xrt_ini.h"
	//Библиотека leonhard x64 xrt
	#include "gpc_defs.h"
	#include "leonhardx64_xrt.h"
	#include "gpc_handlers.h"

	int main(int argc, char** argv)
	{

		// Приложение запскается с параметрами: <xclbin> <sw_kernel>
		// <xclbin>    - путь к бинарному файлу прошивки ПЛИС Ultrascale+ с проектом Леонард Эйлер
		// <sw_kernel> - путь к бинарному файлу sw_kernel в формате rawbinary

		unsigned int err = 0;
		unsigned int cores_count = 0;
		float LNH_CLOCKS_PER_SEC;

		//Использование макроса __foreach_core для определения количества доступных ядер
		__foreach_core(group, core) cores_count++;

		//Проверка передаваемых параметров
		if (argc < 3) {
			usage();
			throw std::runtime_error("FAILED_TEST\nNo xclbin or sw_kernel specified");
		}

		//Инициализация карты #0 и конфигурирование карты ускорителя
		leonhardx64 lnh_inst = leonhardx64(0,argv[1]);

		//Иницилиация программных ядер во всех GPC
		__foreach_core(group, core)
		{
			lnh_inst.load_sw_kernel(argv[2], group, core);
		}

		/*
		 *
		 * Чтение номера версии и статуса из микропроцесосра lnh64 каждого GPC
		 *
		 */

		__foreach_core(group, core)
		{
			printf("Group #%d \tCore #%d\n", group, core);
			// Запуск обработчика get_version() в sw_kernel
			lnh_inst.gpc[group][core]->start_sync(__event__(get_version));
			// Получение сообщения о номере версии (метод класса leonhardx64.mq_receive())
			printf("\tSoftware Kernel Version:\t0x%08x\n", lnh_inst.gpc[group][core]->mq_receive());
			// Запуск обработчика get_lnh_status_high() в sw_kernel
			lnh_inst.gpc[group][core]->start_sync(__event__(get_lnh_status_high));
			// Получение сообщения со значением старшей части регистра статуса 
			printf("\tLeonhard Status Register:\t0x%08x", lnh_inst.gpc[group][core]->mq_receive());
			// Получение сообщения со значением младшей части регистра статуса 
			lnh_inst.gpc[group][core]->start_sync(__event__(get_lnh_status_low));
			printf("_%08x\n", lnh_inst.gpc[group][core]->mq_receive());
		}


		/*
		 *
		 * Измерение тактовой частоты GPC[0]
		 *
		 */

		// Запуск обработчика frequency_measurement() в sw_kernel
		lnh_inst.gpc[0][LNH_CORES_LOW[0]]->start_async(__event__(frequency_measurement));
		// Команда обмена синхронизирующими сообщениями
		lnh_inst.gpc[0][LNH_CORES_LOW[0]]->sync_with_gpc(); // Start measurement
		// Задержка 1 секунда
		sleep(1);
		// Команда обмена синхронизирующими сообщениями
		lnh_inst.gpc[0][LNH_CORES_LOW[0]]->sync_with_gpc(); // Start measurement
		// Ожидание завершения работы обработчика
		lnh_inst.gpc[0][LNH_CORES_LOW[0]]->finish();
		// Чтение сообщения из очереди сообщений
		LNH_CLOCKS_PER_SEC = (float)lnh_inst.gpc[0][LNH_CORES_LOW[0]]->mq_receive();
		printf("Leonhard clock frequency (LNH_CF): %u MHz\n", LNH_CLOCKS_PER_SEC / 1000000);

		exit(0);
	}
``` 

Для представленного листинга должен быть также создан и скомпилирован ответный код для микропроцессора riscv32im, который будет работать в составе гетерогенного ядра обработки графов.
В коде должна быть реализована логика установки состояния ядра: одно из двух состояний IDLE или BUSY. Также разработчиком должы быть реализованы обработчики вызываемых из Хост-подсистемы функций get_version(), get_lnh_status_high(), get_lnh_status_low(), frequency_measurement().

Номер обработчика может быть задан явным обраом в Xост и sw_kernel частях, однако удобнее использовать механизм автоматической нумерации обработчиков на основе макросов С. Для этого мы будем использовать файл gpc_handkers.h, который должен быть включен как в проект Хоста, так и в проект sw_kernel:

```
	#ifndef DEF_HANDLERS_H_
	#define DEF_HANDLERS_H_
	#define DECLARE_EVENT_HANDLER(handler) \
	            const unsigned int event_ ## handler =__LINE__; \
	            void handler ();
	#define __event__(handler) event_ ## handler
	//  Event handlers declarations by declaration line number!!! 
	DECLARE_EVENT_HANDLER(frequency_measurement);
	DECLARE_EVENT_HANDLER(get_lnh_status_low);
	DECLARE_EVENT_HANDLER(get_lnh_status_high);
	DECLARE_EVENT_HANDLER(get_version);
	#endif
```  

Таким образом, условное имя обработчика ставится в однозначное соответствие номеру строки, в которой он объявлен в файле gpc_handlers.h.

В результате получим следующий код основного модуля sw_kernel

```
	#include <stdlib.h>
	#include "lnh64.h"
	#include "gpc_io_swk.h"
	#include "gpc_handlers.h"

	#define VERSION 26
	#define DEFINE_LNH_DRIVER
	#define DEFINE_MQ_R2L
	#define DEFINE_MQ_L2R
	
	// Объявление структур для доступа к ресурсам микропроцессора lnh64
	extern lnh lnh_core;
	// Объявление структур для доступа к глобальной и внешней памяти, и очередям сообщений
	extern global_memory_io gmio;
	volatile unsigned int event_source;

	int main(void) {
	    /////////////////////////////////////////////////////////
	    //          Основной цикл запуска обработчиков
	    /////////////////////////////////////////////////////////
	    //Инициализация микропроцессорного ядра lnh64
	    lnh_init();
	    //Инициализация очередей host2gpc и gpc2host
	    gmio_init(lnh_core.partition.data_partition);
	    for (;;) {
	        //Ожидание события start
	        while (!gpc_start());
	        //Переход в режим BUSY и разрешение ввода/вывода
	        set_gpc_state(BUSY);
	        //Получение номера обработчика 
	        event_source = gpc_config();
	        //Вызов обработчика по номеру
	        switch(event_source) {
	            case __event__(frequency_measurement) : frequency_measurement(); break;
	            case __event__(get_lnh_status_low) : get_lnh_status_low(); break;
	            case __event__(get_lnh_status_high) : get_lnh_status_high(); break;
	            case __event__(get_version): get_version(); break;
	        }
	        //Переход в режим IDLE и запрет вводы/вывода
	        set_gpc_state(IDLE);
	        while (gpc_start());
	    }
	}
	    
	//-------------------------------------------------------------
	//      Глобальные переменные (для сокращения объема кода)
	//-------------------------------------------------------------
	    
	        u64 TSC_start;
	        u64 TSC_stop;
	        u32 interval;

	//-------------------------------------------------------------
	//      Измерение тактовой частоты GPN
	//-------------------------------------------------------------
	 
	void frequency_measurement() {

	        sync_with_host();
	        lnh_sw_reset();
	        lnh_rd_reg32_byref(TSC_LOW,&TSC_start);
	        sync_with_host();
	        lnh_rd_reg32_byref(TSC_LOW,&TSC_stop);
	        interval = TSC_stop-TSC_start;
	        mq_send(interval);
	}

	//-------------------------------------------------------------
	//      Получить версию микрокода 
	//-------------------------------------------------------------
	 
	void get_version() {

	        mq_send(VERSION);
	}

	//-------------------------------------------------------------
	//      Получить регистр статуса LOW Leonhard 
	//-------------------------------------------------------------
	 
	void get_lnh_status_low() {

	        lnh_rd_reg32_byref(LNH_STATE_LOW,&lnh_core.result.status);
	        mq_send(lnh_core.result.status);
	}

	//-------------------------------------------------------------
	//      Получить регистр статуса HIGH Leonhard 
	//-------------------------------------------------------------
	 
	void get_lnh_status_high() {

	        lnh_rd_reg32_byref(LNH_STATE_HIGH,&lnh_core.result.status);
	        mq_send(lnh_core.result.status);
	}
``` 


# 3. Практическа часть <a name="3"></a>

## Лабораторная работа №1. Разработка и отладка программ в вычислительном комплексе Тераграф с помощью leonhard x64 xrt <a name="3_1"></a>





## Лабораторная работа №2. Обработка и визуализация графов в вычислительном комплексе Тераграф <a name="3_2"></a>





## Командный практикум. Обработка и визуализация графов в вычислительном комплексе Тераграф <a name="3_3"></a>

 



